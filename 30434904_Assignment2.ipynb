{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Assessment 2\n",
    "#### Student Name: Sarath Gopinathan\n",
    "#### Student ID: 30434904\n",
    "\n",
    "Date: 06/10/2020\n",
    "\n",
    "Version: 1.4\n",
    "\n",
    "Environment: Python 3.8.5 and Jupyter notebook\n",
    "\n",
    "Libraries used: \n",
    "* pandas to read and perform actions on the given files\n",
    "* numpy to perform calculations\n",
    "* SentimentIntensityAnalyzer to perform sentiment analysis\n",
    "* math to find distance between 2 lat longs\n",
    "* numpy to solve linear equations\n",
    "* itertools to get all the permutations and combinations of products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [Importing packages](#import_packages)\n",
    "* [Reading data](#reading_data)\n",
    "* [Gathering data to complete tasks](#gather_data)\n",
    "* [Adding the missing data](#missing_data)\n",
    "* [Types of anomalies](#types_of_anomalies)\n",
    "* [Process of cleaning the dirty data](#process)\n",
    "* [Identifying and removing outliers](#outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages <a class=\"anchor\" id=\"import_packages\"></a>\n",
    "\n",
    "Importing all packages required to complete the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import math\n",
    "import numpy as np\n",
    "import itertools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data <a class=\"anchor\" id=\"reading_data\"></a>\n",
    "\n",
    "The package pandas is used to read the the input files. We can also view basic info of all the files(non-null counts, datatypes) using the package. This gives us insights about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data = pd.read_csv(\"30434904_dirty_data.csv\")\n",
    "outliers = pd.read_csv(\"30434904_outlier_data.csv\")\n",
    "missing = pd.read_csv(\"30434904_missing_data.csv\")\n",
    "warehouses = pd.read_csv(\"warehouses.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouses.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering data to complete tasks <a class=\"anchor\" id=\"gather_data\"></a>\n",
    "\n",
    "Gathering required data which will be used all along the process to complete all the tasks.\n",
    "\n",
    "1. Identifying unique products\n",
    "2. Calculating prices for each of the unique products.\n",
    "3. Finding closest warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to create the list of coefficients to help solve the linear equations\n",
    "def create_lhs(cart, products):\n",
    "    \n",
    "    current_lhs = []\n",
    "    \n",
    "    for i in range(0, len(products)):\n",
    "        \n",
    "        current_lhs.append(0)\n",
    "       \n",
    "    for i in range(0, len(cart)):\n",
    "        \n",
    "        tag = products.index(cart[i][0])\n",
    "        \n",
    "        current_lhs[tag] = cart[i][1]\n",
    "    \n",
    "    return current_lhs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to find unique products\n",
    "def find_products(data):\n",
    "    \n",
    "    products = []\n",
    "\n",
    "    for i in range (0, data['order_id'].count()):\n",
    "\n",
    "            cart = eval(data.iloc[i][4])\n",
    "\n",
    "            for j in range (0, len(cart)):\n",
    "\n",
    "                if(cart[j][0] not in products):\n",
    "\n",
    "                    products.append(cart[j][0])\n",
    "\n",
    "    return products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to find unique products and their respective price . Returns a tuple with product and price in it.\n",
    "def get_products_with_price():\n",
    "\n",
    "    products_with_price = []\n",
    "    lhs = []\n",
    "    rhs = []\n",
    "\n",
    "    products = find_products(outliers)\n",
    "\n",
    "    for i in range(0, len(products)):\n",
    "\n",
    "        flag = 0\n",
    "\n",
    "        for j in range (0, outliers['order_id'].count()):\n",
    "\n",
    "            cart = eval(outliers.iloc[j][4])\n",
    "\n",
    "            for k in range(0, len(cart)):\n",
    "\n",
    "                if(cart[k][0] == products[i]):\n",
    "\n",
    "                    current_lhs = create_lhs(cart, products)\n",
    "\n",
    "                    if(current_lhs not in lhs):\n",
    "\n",
    "                        lhs.append(current_lhs)\n",
    "                        rhs.append(outliers.iloc[j][5])\n",
    "                        flag = 1\n",
    "                        break\n",
    "\n",
    "            if(flag == 1):\n",
    "                flag = 0\n",
    "                break\n",
    "\n",
    "    finalLHS = np.array(lhs)\n",
    "    finalRHS = np.array(rhs)\n",
    "\n",
    "    price_list = np.linalg.inv(finalLHS).dot(finalRHS)\n",
    "\n",
    "    for i in range(0,len(products)):\n",
    "\n",
    "        product_price = (products[i], round(price_list[i]))\n",
    "        products_with_price.append(product_price)\n",
    "\n",
    "    return products_with_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to calculate and find the closest warehouse. Returns the closest warehouse and the distance as a dictionary.\n",
    "def calc_distance(lat1, long1):\n",
    "    \n",
    "    closest_warehouse = {}\n",
    "    \n",
    "    for i in range (0, warehouses['names'].count()):\n",
    "\n",
    "        lat2 = warehouses.iloc[i][1]\n",
    "        long2 = warehouses.iloc[i][2]\n",
    "    \n",
    "        # Converts lat & long to spherical coordinates in radians.\n",
    "        degrees_to_radians = math.pi/180.0\n",
    "\n",
    "        # phi = 90 - latitude\n",
    "        phi1 = (90.0 - lat1)*degrees_to_radians\n",
    "        phi2 = (90.0 - lat2)*degrees_to_radians\n",
    "\n",
    "        # theta = longitude\n",
    "        theta1 = long1*degrees_to_radians\n",
    "        theta2 = long2*degrees_to_radians\n",
    "\n",
    "        cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) + math.cos(phi1)*math.cos(phi2))\n",
    "        \n",
    "        #computes distance value using formula\n",
    "        distance = round(math.acos(cos)*6378,4) #radius of the earth in km\n",
    "        \n",
    "        if(i == 0):\n",
    "            closest_warehouse = {\"warehouse\": warehouses.iloc[i][0], \"distance\": distance}\n",
    "        else:\n",
    "            if(closest_warehouse.get(\"distance\") > distance):\n",
    "                closest_warehouse = {\"warehouse\": warehouses.iloc[i][0], \"distance\": distance}\n",
    "                \n",
    "    return closest_warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to find the product price for the respective product\n",
    "def find_prod_price(prod):\n",
    "    \n",
    "    prod_list = get_products_with_price()\n",
    "    \n",
    "    for i in range(0,len(prod_list)):\n",
    "        \n",
    "        if(prod_list[i][0] == prod):\n",
    "            return prod_list[i][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used to calculate the order_price of the cart\n",
    "def calculate_order_price(cart):\n",
    "    \n",
    "    total_order_price = 0\n",
    "    \n",
    "    for i in range(0, len(cart)):\n",
    "        \n",
    "        prod = cart[i][0]\n",
    "        \n",
    "        price = find_prod_price(prod)\n",
    "        \n",
    "        total_order_price += price * cart[i][1]\n",
    "        \n",
    "#     print(total_order_price)\n",
    "        \n",
    "    return total_order_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the missing data <a class=\"anchor\" id=\"missing_data\"></a>\n",
    "\n",
    "There are a lot of missing data in the missing data file. We check for cells with empty values and then fill them up with respective values after performing calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process to add the missing data <a class=\"anchor\" id=\"process_missing_data\"></a>\n",
    "\n",
    "Identify all the empty cells in each of the columns and fill them up with the respective values after calculating them. The following is the process to fill all the empty cells with their respective values.\n",
    "\n",
    "1. Since there are missing values under order_price, we need to calculate all the products and their respective prices.\n",
    "2. Calculate the distance between the customer location and all the three warehouse and check the distance_to_nearest_warehouse and match the respective warehouse and fill all the empty nearest_warehouse cells with the value.\n",
    "3. Calculate order_price by performing calculations according to shopping_cart after finding the product prices and fill the empty cells with the value.\n",
    "4. Calculate delivery_charges by using the formula (order_total - (order_price - (order_price*(coupon_discount/100)))) and fill all the empty cells.\n",
    "5. Calculate the order_total by using the formula (order_price + ((order_price - order_price*(coupon_discount/100))) + delivery_charges) and fill all the empty cells.\n",
    "6. Calculate the distance between the customer location and all the three warehouse and check for the smallest one and fill the empty distance_to_nearest_warehouse cell with the respective values.\n",
    "7. Perform sentiment analysis on latest_customer_review and fill empty is_customer_happy value cells with the respective value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gets the prices\n",
    "individual_products = get_products_with_price()\n",
    "\n",
    "#changes data whereever it mismatches\n",
    "for i in range (0, missing['order_id'].count()):\n",
    "    \n",
    "    closest_warehouse = calc_distance(missing.iloc[i][7], missing.iloc[i][8])   \n",
    "    \n",
    "    if(pd.isnull(missing.iloc[i][3])): \n",
    "        missing.at[i,'nearest_warehouse'] = closest_warehouse.get(\"warehouse\")\n",
    "        if(pd.isnull(missing.iloc[i][13])): \n",
    "            missing.at[i,\"distance_to_nearest_warehouse\"] = closest_warehouse.get(\"distance\")\n",
    "    \n",
    "    if(pd.isnull(missing.iloc[i][5])):\n",
    "        \n",
    "        order_price = calculate_order_price(eval(missing.iloc[i][4]))\n",
    "        missing.at[i,\"order_price\"] = order_price\n",
    "        \n",
    "    if(pd.isnull(missing.iloc[i][6])):\n",
    "        \n",
    "        delivery_charges = missing.iloc[i][10] - (missing.iloc[i][5] - ((missing.iloc[i][5]*missing.iloc[i][9])/100))\n",
    "        missing.at[i,\"delivery_charges\"] = delivery_charges\n",
    "        \n",
    "    if(pd.isnull(missing.iloc[i][10])):\n",
    "        \n",
    "        order_total = (missing.iloc[i][5] - (missing.iloc[i][5]*(missing.iloc[i][9]/100))) + missing.iloc[i][6]\n",
    "        missing.at[i,\"order_total\"] = order_total\n",
    "        \n",
    "    if(pd.isnull(missing.iloc[i][15])):\n",
    "        \n",
    "        analyser = SentimentIntensityAnalyzer()\n",
    "        score = analyser.polarity_scores(dirty_data.iloc[i][14])\n",
    "        \n",
    "        if(score.get('compound') >= 0.05):\n",
    "        \n",
    "            missing.at[i,\"is_happy_customer\"] = 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            missing.at[i,\"is_happy_customer\"] = 0   \n",
    "\n",
    "#writing to new file\n",
    "missing.to_csv(r'30434904_missing_data_solution.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of anomalies <a class=\"anchor\" id=\"types_of_anomalies\"></a>\n",
    "\n",
    "Data Anomalies can be classified at a high level into three categories:\n",
    "\n",
    "* Syntactical Anomalies: describe characteristics concerning the format and values used for representation of the entities. Syntactical anomalies such as: lexical errors, domain format errors, syntactical error and irregularities.\n",
    "\n",
    "* Semantic Anomalies: hinder the data collection from being a comprehensive and non-redundant representation of the mini-world. These types of anomalies include: Integrity constraint violations, contradictions, duplicates and invalid tuples.\n",
    "\n",
    "* Coverage Anomalies: decrease the amount of entities and entity properties from the mini-world that are represented in the data collection. Coverage anomalies are categorised as: missing values and missing tuples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process of cleaning the dirty data <a class=\"anchor\" id=\"process\"></a>\n",
    "\n",
    "There are different anomolies that are present in the given data. We know that 3 columns have correct data - coupon_discount, delivery_chargesand quantity values in the shopping_cart. Thus we should check each and every other column to confirm if an anomaly exists. We also know that there can excist only 1 anomaly in a row. The following process will be followed to clean the given data :\n",
    "\n",
    "1. Check if all the order ids are unique.\n",
    "2. All date must follow the format YYYY-MM-DD\n",
    "3. Check if the customer lat long are correct. If not, switch them and check if they are correct.\n",
    "4. Check if the seasons fall under - Spring, Summer, Autumn, Winter and check if the date correctly matches.\n",
    "5. Perform sentiment analysis on latest_customer_review and modify is_customer_happy value if there is a change.\n",
    "6. Check if the nearest warehouse and the distance is correct by calculating the distance between customer location and the warehouse locations.\n",
    "7. Find the price of each item in shopping cart using the outlier dataset as it has no anomalies in it.\n",
    "8. Now check if the order price matches the shopping_cart and make changes as required.\n",
    "9. After this, verify the order_total by substituting the values in the given formul and make changes if required.\n",
    "10. Check if is_expedited is TRUE or FALSE using the given conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if all the order_ids are unique.\n",
    "count = dirty_data['order_id'].count()\n",
    "uniq = dirty_data['order_id'].nunique()\n",
    "\n",
    "if(count == uniq):\n",
    "    print(\"All order ids are unique\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1 Cleaning (Date, Season, is_happy_customer)\n",
    "\n",
    "Here we check the date format, season value and we perform sentiment analysis to confirm if the value in is_happy_customer is correct. If they are found to be wrong, the changes will be made and the value in changes_made will be changed to 1 from 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data.describe()\n",
    "dirty_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data[\"is_happy_customer\"] = dirty_data[\"is_happy_customer\"].astype(int)\n",
    "for i in range (0, dirty_data['order_id'].count()):\n",
    "    \n",
    "#     correcting date format\n",
    "    if(\"-\" in dirty_data.iloc[i][2][0:4]):\n",
    "        dirty_data.at[i,'date'] = pd.to_datetime(dirty_data.iloc[i][2]).strftime('%Y-%m-%d')\n",
    "    \n",
    "    month = dirty_data.iloc[i][2][5:7]\n",
    "        \n",
    "#     correcting season if it doesnt match the month\n",
    "    if(month == \"09\" or month == \"10\" or month == \"11\"):\n",
    "        if(dirty_data.iloc[i][11] != \"Spring\"):\n",
    "            dirty_data.at[i,'season'] = \"Spring\"         \n",
    "    elif(month == \"12\" or month == \"01\" or month == \"02\"):\n",
    "        if(dirty_data.iloc[i][11] != \"Summer\"):\n",
    "            dirty_data.at[i,'season'] = \"Summer\"\n",
    "    elif(month == \"03\" or month == \"04\" or month == \"05\"):\n",
    "        if(dirty_data.iloc[i][11] != \"Autumn\"):\n",
    "            dirty_data.at[i,'season'] = \"Autumn\"\n",
    "    elif(month == \"06\" or month == \"07\" or month == \"08\"):\n",
    "        if(dirty_data.iloc[i][11] != \"Winter\"):\n",
    "            dirty_data.at[i,'season'] = \"Winter\"\n",
    "    \n",
    "#     performing sentiment analysis to correct the is_happy_customer column values\n",
    "    analyser = SentimentIntensityAnalyzer()\n",
    "    score = analyser.polarity_scores(dirty_data.iloc[i][14])\n",
    "    \n",
    "    if(score.get('compound') >= 0.05):\n",
    "        \n",
    "        if(dirty_data.iloc[i][15] == 0):\n",
    "            dirty_data.at[i,\"is_happy_customer\"] = 1  \n",
    "            \n",
    "    else:\n",
    "            \n",
    "        if(dirty_data.iloc[i][15] == 1):\n",
    "            dirty_data.at[i,\"is_happy_customer\"] = 0\n",
    "            \n",
    "phase_one_clean = dirty_data\n",
    "\n",
    "phase_one_clean.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2 Cleaning (Nearest warehouse, customer location, distance to nearest warehouse)\n",
    "\n",
    "Initially we check if the customer lat long are valid.If they are not valid, check by swtiching the lat and long. Next, from the warehouse details and the given customer lat long, we compute the nearest warehouse and the distance to the nearest warehouse. Make the changes as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0, phase_one_clean['order_id'].count()):\n",
    "    \n",
    "#     checking if the lat longs are switched\n",
    "    if(phase_one_clean.iloc[i][7] > 90 or phase_one_clean.iloc[i][7] < -90):\n",
    "        temp_lat = phase_one_clean.iloc[i][7]\n",
    "        phase_one_clean.at[i,'customer_lat'] = phase_one_clean.iloc[i][8]\n",
    "        phase_one_clean.at[i,'customer_long'] = temp_lat\n",
    "    elif(phase_one_clean.iloc[i][8] > 180 or phase_one_clean.iloc[i][8] < -180):\n",
    "        temp_lat = phase_one_clean.iloc[i][7]\n",
    "        phase_one_clean.at[i,'customer_lat'] = phase_one_clean.iloc[i][8]\n",
    "        phase_one_clean.at[i,'customer_long'] = temp_lat\n",
    "    \n",
    "#     Calculates closest warehouse. Contains warehouse and distance.\n",
    "    closest_warehouse = calc_distance(phase_one_clean.iloc[i][7], phase_one_clean.iloc[i][8])\n",
    "    \n",
    "#     check if the distance is not equal to the closest warehouse distance and change if required.\n",
    "    if(phase_one_clean.iloc[i][13] != closest_warehouse.get(\"distance\")):\n",
    "        phase_one_clean.at[i,'distance_to_nearest_warehouse'] = closest_warehouse.get(\"distance\")\n",
    "        \n",
    "#     check if the warehouse is same as closest warehouse and change if required.        \n",
    "    if(phase_one_clean.iloc[i][3] != closest_warehouse.get(\"warehouse\")):\n",
    "        phase_one_clean.at[i,'nearest_warehouse'] = closest_warehouse.get(\"warehouse\")\n",
    "\n",
    "        \n",
    "phase_two_clean = phase_one_clean\n",
    "\n",
    "phase_two_clean.head(60)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3 Cleaning (shopping cart, order price, order total)\n",
    "\n",
    "Since outliers data doesnt have any anomalies, we find each items price from the outliers data by solving linear equations. We then check if the shopping cart, order price match. Change the values accordingly and then calculate order total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perm_cart(cart, order_price):\n",
    "    \n",
    "    prods = find_products(outliers)\n",
    "#     prods = []\n",
    "    cart_size = len(cart)       \n",
    "    perm_list = list(itertools.permutations(prods,cart_size))\n",
    "    \n",
    "    for i in range (0,len(perm_list)):\n",
    "        \n",
    "        current_cart = []\n",
    "        \n",
    "        for j in range (0, len(perm_list[i])):\n",
    "        \n",
    "            temp = (perm_list[i][j], cart[j][1])\n",
    "\n",
    "            current_cart.append(temp)\n",
    "\n",
    "        current_cart_price = calculate_order_price(current_cart)\n",
    "        \n",
    "        if(current_cart_price == order_price):\n",
    "                return current_cart\n",
    "    \n",
    "    return []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products_with_price = get_products_with_price()\n",
    "products_with_price\n",
    "\n",
    "for i in range (0, phase_two_clean['order_id'].count()):\n",
    "    \n",
    "    order_price = calculate_order_price(eval(phase_two_clean.iloc[i][4]))\n",
    "    \n",
    "    if(phase_two_clean.iloc[i][5] != order_price):\n",
    "              \n",
    "        cart = perm_cart(phase_two_clean.iloc[i][4], phase_two_clean.iloc[i][5])\n",
    "        \n",
    "        if(len(cart) == 0):\n",
    "            \n",
    "            phase_two_clean.at[i,'order_price'] = order_price\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            phase_two_clean.at[i,'shopping_cart'] = cart\n",
    "            \n",
    "    current_order_total = (phase_two_clean.iloc[i][5] - (phase_two_clean.iloc[i][5]*(phase_two_clean.iloc[i][9]/100))) + phase_two_clean.iloc[i][6]\n",
    "    phase_two_clean.at[i,\"order_total\"] = current_order_total\n",
    "\n",
    "phase_three_clean = phase_two_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the clean file into a new csv \n",
    "\n",
    "We now write the entire dataframe as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phase_three_clean.to_csv(r'30434904_dirty_data_solution.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying and removing outliers <a class=\"anchor\" id=\"outliers\"></a>\n",
    "\n",
    "Identifying and removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.boxplot(column='delivery_charges',figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.boxplot(figsize=(10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers['delivery_charges'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying q1 for 0.25 - lq\n",
    "q1 = outliers['delivery_charges'].quantile(0.25)\n",
    "#identifying q1 for 0.75 - hq\n",
    "q3 = outliers['delivery_charges'].quantile(0.75)\n",
    "iqr = q3-q1 #Interquartile range\n",
    "fence_low  = q1-1.5*iqr\n",
    "fence_high = q3+1.5*iqr\n",
    "without_outliers = outliers.loc[(outliers['delivery_charges'] > fence_low) & (outliers['delivery_charges'] < fence_high)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing the clean file into a new csv \n",
    "\n",
    "We now write the entire dataframe as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_outliers.to_csv(r'30434904_outliers_data_solution.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
